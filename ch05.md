# 第5章 并发及并行

> 并发(concurrency)的意思是说,计算机似乎是在同一时间做着很多不同的事.但其实是操作系统在各程序之间迅速切换,使其都有机会运行在这一个处理器上面.
>
> 并行(parallelism)的意思则是说,计算机确实是在同一时间做着很多不同的事.
>
> 在同一个程序内部,并发是一种工具,它使程序员可以更加方便的解决特定类型的问题.在并发程序中,不同的执行路径都能够以某种方式向前推进,而这种方式,使人感觉那些路径可以在同一时间独立地运行.
>
> 并行与并发的关键区别,就在于能不能提速.某程序若是并行程序,其中有两条不同的执行路径都在平行的向前推进,则总任务的执行时间会减半,执行速度会变为普通程序的两倍.反之,假如该程序是并发程序,那么它即使可以用看似平行的方式分别执行多条路径,也依然不会使总任务的执行速度得到提升.

## 第36条: 用subprocess模块来管理子进程

> 由python所启动的多个子进程,是可以平行运作的,这使得我们能够在python程序中充分利用电脑中的全部CPU核心,从而尽量提成程序的处理能力(throughput,吞吐量).虽然python解释器本事可能会受限于CPU(参见本书第37条),但是开发者依然可以用python顺畅的驱动并协调那些耗费CPU的工作任务.

```python
# 用Popen构造器来启动进程
proc = subprocess.Popen(['echo', 'Hello from the child!'],stdout=subprocess.PIPE)
out, err = proc.communicate() # 用communicate方法读取子进程的输出信息,并等待其终止
print(out.decode('utf-8'))

# 子进程独立于父进程运行;父进程为python解释器;一边定期查询子进程的状态,一边处理其他事务
proc = subprocess.Popen(['sleep','0.3'])
while proc.poll() is None:
    print('Working...')
    
print('Exit status', proc.poll())

'''
把子进程从父进程中剥离(decouple,解耦),意味着父进程可以随意运行很多条平行的子进程.为了实现这一点,我们可以先把所有的子进程都启动起来.
'''
def run_sleep(period):
    proc = subprocess.Popen(['sleep', str(period)])
    return proc

start = time()
procs = []
for _ in range(10):
    proc = run_sleep(0.1)
    procs.append(proc)
    
#然后通过communicate方法,等待这些子进程完成其I/O工作并终结
for proc in procs:
    proc.communicate()
end = time()
print('Finished in %.3f seconds' % (end-start))
'''
以上代码的结论:父进程会把所有的子进程都启动起来,而不是把这些子进程逐个运行,因为如果是逐个运行的化,完成时间应该是1秒左右,而不是0.1秒左右.
'''

'''
我们可以从python程序项子进输送数据,或者获取子进程的输出信息.例如:用命令行工具openssl加密一些数据
'''
def run_openssl(data):
    env = os.environ.copy()
    env['password'] = b'\xe24U\n\xd0Q13S\x11'
    proc = subprocess.Popen(
        ['openssl','enc','-des3','-pass','env:password'],
        env=env,
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE
    )
    proc.stdin.write(data)
    proc.stdin.flush()  # Ensure the child gets input
    return proc

procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_openssl(data)
    procs.append(proc)
    
for proc in procs:
    out, err = proc.communicate()
    print(out[-10:])
    
'''
另外我们还可以像UNIX管道那样,把子进程搭建链条(chain),即把第一个子进程的输出与第二个子进程的输入联系起来,并以此方式继续拼接下去.
'''
def run_md5(input_stdin):
    proc = subprocess.Popen(
        ['md5sum'],
        stdin = input_stdin,
        stdout=subprocess.PIPE
    )
    return proc
'''
现在启动一套openssl进程,以便加密某些数据,同时启动另一套md5进程,以便根据加密后的输出内容来计算其哈希码
'''
input_procs = []
hash_procs = []
for _ in range(3):
    data = os.urandom(10)
    proc = run_openssl(data)
    input_procs.append(proc)
    hash_proc = run_md5(proc.stdout) #使用上一个进程的输出作为当前进程的输入
    hash_procs.append(hash_proc)

for proc in input_procs:
    proc.communicate()

for proc in hash_procs:
    out, err = proc.communicate()
    print(out.strip())
    
'''
    如果担心子进程一直不终止,或担心它的输出管道或输入管道由于某些原因发生阻塞,那么可以给communicate方法传入timeout参数.
    若子进程在指定时间内没有给出响应,那么communicate会抛出异常,我们可以在处理异常的时候,终止出现意外的子进程
'''
proc = run_sleep(10)
try:
    proc.communicate(timeout=0.1)
except subprocess.TimeoutExpired:
    proc.terminate()
    proc.wait()
print('Exit status', proc.poll()) #输出子进程的退出状态
```



> timeout参数仅在python3.3及后续版本中有效.对于之前的版本,我们需要用内置的select模块来处理proc.stdin,proc.stdout,proc.stderr以确保I/O操作的超时机制能够生效.



### 要点

- 可以用subprocess模块运行子进程,并管理其输入流与输出流
- python解释器能够平行的运行多个子进程,这使得开发折可以充分利用cpu的处理能力
- 可以给communicate方法传入timeout参数,以避免子进程死锁或失去响应(hanging,挂起)

## 第37条: 可以用线程来执行阻塞式I/O,但不要用它做平行计算

> 标准的python实现叫做Cpython.CPython分两步来运行python程序:
>
> 首先,把文本形式的源代码解析并编译成字节码.
>
> 然后,用一种基于栈的解释器来运行这份字节码.
>
> 执行python程序时,字节码解释器必须保持协调一致的状态.python采用GIL(global interpreter lock,全局解释器锁)机制来确保这种同步性.
>
> GIL实际上是一把互斥锁(mutex, 互斥体),用以防止CPython受到占先式多线程切换操作的干扰.
>
> 所谓占先式多线程切换,是指某个线程可以通过打断另外一个线程的方式,来获取程序控制权.
>
> 而GIL可保证不会出现这种打断的操作,使得每条字节码指令均能够正确的与CPython实现及其C语言扩展模块协同运作.
>
> GIL的缺陷:用C++或Java编程的时候,可以同时执行多条线程,以充分利用计算机所配备的多个CPU核心.Python程序尽管也支持多线程,但由于受到GIL保护,所以同一时刻,只有一条线程可以向前执行.也就是说在多核计算机上无法实现多线程并行计算.

```python
def factorize(number):
    for i in range(1, number+1):
        if number % i == 0:
            yield i
            
numbers = [2139079, 1214759, 1516637, 1852285]
start = time()
for number in numbers:
    list(factorize(number))
end = time()
print('Took %.3f seconds' % (end-start))

'''
使用python的多线程库
'''
from threading import Thread
class FactorizeThead(Thread):
    def __init__(self, number):
        super().__init__()
        self.number = number
        
    def run(self):
        self.factors = list(factorize(self.number))
        
start = time()
threads = []
for number in numbers:
    thread = FactorizeThread(number)
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

end = time()
print('Took %.3f seconds' % (end-start))
'''
通过以上两种不同的实现,我们可以发现消耗的时间相差不多,甚至使用多线程的实现方式耗费的时间比第一种方式稍微多一点点.

结论:CPython解释器中的多线程程序受到了GIL的影响.

通过一些其他方式,我们确实可以令CPython解释器利用CPU的多个内核,但是那些方式所使用的并不是标准的Thread类(参见本书的第41条),而且还需要开发者编写较多的代码.那么python的多线程机制的作用是什么呢?

其一,多线程使得程序看上去好像能够在同一时间做许多事情.
其二,多线程可用于处理阻塞式I/O操作,Python在执行某些系统调用时,会触发此类操作.执行系统调用,是指Python程序请求计算机的操作系统与外界环境相交互,以满足程序的需求.
'''

'''
举例:通过串行端口(serial port,简称串口)发送信号,以便远程控制一架直升飞机.以下使用系统调用select来模拟这项活动;该函数请求操作系统阻塞0.1秒,然后把控制权还给程序,这种效果与通过同步串口来发送信号是类似的.
'''
import select
def slow_systemcall():
    select.select([],[],[],0.1)

start = time()
for _ in range(5):
    slow_systemcall() #此处程序的主线程会卡在select系统调用那里.
end = time()
print('Took %.3f seconds' % (end-start))
'''
以上代码的致命缺点:因为发送信号的同时,程序必须算出直升飞机接下来要移动到的地点,否则肺经可能就会撞毁.如果要同时执行阻塞式I/O操作和计算操作,那就应该考虑把系统调用放到其他线程里面.
'''
def compute_helicopter_location(index):
    pass

for i in range(5):
    compute_helicopter_location(i)
    
for thread in threads:
    thread.join()
end = time()
print('Took %.3f seconds' % (end-start))
'''
除了线程,还有其他一些方式,也能处理阻塞式的I/O操作,例如,内置的asyncio模块等.虽然那些方式都有着非常显著的有点,但它们要求开发者必须花些功夫,将代码重构成另外一种执行模型(参见第40条).
'''
```

### 要点

- 因为受到全局解释锁(GIL)的限制,所以多条Python线程不能在多个CPU核心上面平行地执行代码
- 尽管受制于GIL,但是Python的多线程功能依然很有用,它可以轻松的模拟出同一时刻执行多项任务的效果
- 通过Python线程,我们可以平行地执行多个系统调用,这使得程序能够在执行阻塞式I/O操作的同时,执行一些运算操作.

## 第38条: 在线程中使用Lock来方式数据竞争

> GIL并不会保护开发者自己所编写的代码.同一时刻固然只能有一个python线程得以运行,但是,当这个线程正在操作某个数据结构时,其他线程可能会打断它,也就是说,python解释器在执行两个连续的字节码指令时,其他线程可能会在中途突然插进来.如果开发者尝试从多个线程中同时访问某个对象,那么上述情形就会引发危险的结果.

```python
class Counter(object):
    def __init__(self):
        self.count = 0
        
    def increment(self, offset):
        self.count += offset
        
        
# 查询传感器读数的时候会发生阻塞式I/O操作,所以要给每个传感器分配自己的工作线程.每采集一次读数,工作线程就会给Counter对象的value值加1,然后继续采集,直至完成全部采样操作
def worker(sensor_index, how_many, counter): #how_many表示样本总数
    for _ in range(how_many):
        counter.increment(1)
        
#为每个传感器启动一条工作线程,然后等待它们完成各自的采样工作
def run_threads(func, how_many, counter):
    threads=[]
    for i in range(5):
        args = (i, how_many, counter)
        thread = Thread(target=func, args=args)
        threads.append(thread)
        thread.start()
    for thread in threads:
        thread.join()
        
how_many = 10**5
counter = Counter()
run_threads(worker, how_many, counter)
print('Counter should be %d, found %d' % (5*how_many, counter.count))
'''
最终的结构不是50万,而是少于50万;
原因:为了保证所有的线程都能够公平地执行,GIL会给每个线程分配大致相等的处理器时间.那么有可能出现有一些线程正在执行的时候被暂停(suspend),那么有些计数操作并不是原子操作,当计数操作没有完成之前被暂停,就会出现以上的情况.

+=操作符,实际上GIL会把它拆解成3个独立的操作来完成的:
1. value = getattr(counter, 'count)
2. result = value+offset
3. setattr(counter, 'count', result)

而线程有可能在任意两个操作之间发生线程切换.这种交错执行方式会令线程把旧的value设置给Counter,从而使程序的运行结果出现问题.
比如,运行线程A和线程B,它们的线程切换情况如下:
# Running in Thread A
value_a = getattr(counter, 'count')
#Context switch to Thread B
value_b = value_b +1
setattr(counter, 'count', value_b)
# Context switch back to Thread A
result_a = value_a + 1
setattr(counter, 'count', result_a)
上面的例子中,当线程B执行结束之后,切换回线程A的时候,线程A把线程B刚才对计数器所做的递增效果完全抹去了(这种现象叫做线程A踩踏(stomp)线程B).

为了防止诸如此类的竞态,threading模块提供了一套健壮的工具,使得开发者可以保护自己的数据结构不受破坏.其中最简单的就是Lock类,该类相当于互斥锁
'''
class LockingCounter(object):
    def __init__(self):
        self.lock = Lock()
        self.count = 0
        
    def increment(self, offset):
        with self.lock:
            self.count += offset
```

### 要点

-  python确实有全局解释器锁,但是在编写自己的程序时,依然要设法防止多个线程争用一份数据
- 如果在不加锁的前提下,允许多条线程修改同一个对象,那么程序的数据结构可能会遭到破坏
- 在python内置的threading模块中,有个名叫Lock的类,它用标准的方式实现了互斥锁

## 第39条: 用Queue来协调各线程之间的工作

> 如果python程序同时要执行许多事务,那么开发者经常需要协调这些事务.而较为高效的协调方式为采用函数管道(pipeline).

```python
'''
构建一个照片处理系统,该系统从数码相机里面持续获取照片,调整其尺寸,并将其添加到网络相册中.系统设计:
1. 第一阶段获取新图片
2. 第二阶段把下载好的图片传给缩放函数
3. 第三阶段把缩放后的图片交给上传函数
如何这三个阶段拼接为一条可以并发处理照片的管线呢?
首先,设计一种任务传递方式,以便在管线的不同阶段之间传递工作任务.可以用线程安全的生产者-消费者队列来建模(线程安全的重要性请参阅第38条,deque类的用法请参阅本书第46条)
'''
class MyQueue(object):
    def __init__(self):
        self.items = deque()
        self.lock = Lock()
        
    def put(self, item):
        '''这个代表数码相机,相当于生产者'''
        with self.lock:
            self.items.append(item)
            
    def get(self):
        '''图片处理管线的第一阶段-获取图片,相当于消费者'''
        with self.lock:
            return self.items.popleft()
'''
接下来,我们使用python线程来表示管线的各个阶段
'''
class Worker(Thread):
    def __init__(self, func, in_queue, out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0
        
    def run(self):
        while True:
            self.polled_count += 1
            try:
                item = self.in_queue.get()
            except IndexError:
                sleep(0.01)  # No work to do
            else:
                result = self.func(item)
                self.out_queue.put(result)
                self.work_done += 1
                
download_queue = MyQueue()
resize_queue = MyQueue()
upload_queue = MyQueue()
done_queue = MyQueue()
threads = [
    Worker(download, download_queue, resize_queue),
    Worker(resize, resize_queue, upload_queue),
    Worker(upload, upload_queue, done_queue),
]
for thread in threads:
    thread.start()
for _ in range(1000):
    download_queue.put(object())
    
while len(done_queue.items)<1000:
    pass
'''
问题:
1. 上面的worker类的run方法,会定时的查询输入队列,如果上一阶段迟迟没有完成,那么会造成后面阶段的工作线程会处于饥饿(starve)状态,从而使得工作线程白白浪费CPU实现去执行一些没有用的操作.

2. 另外,为了判断所有任务是否都彻底处理完毕,我们必须再编写一个循环,持续判断done_queue队列中的任务数量.

3. 其次,woker线程的run方法会一直执行循环,即使完成了所有的任务,也无法退出循环.

4. 如果我们要处理的任务非常多,而多个阶段之间的处理速度不一致的情况下,如果第一阶段的处理速度非常快,而第二阶段处理速度非常慢,那么第二阶段的输入队列会占用越来越多的内存,从而导致内存泄露.

使用queue类来弥补自编队列的缺陷;该类能够彻底解决上面提出的那些问题.
1. 无需频繁查询输入队列的状态,因为它的get方法会持续阻塞,直到有新的数据加入
2. 为了解决管线的迟滞问题,用Queue类来限定队列中待处理的最大任务数量,使得相邻的两个阶段,可以通过该队列平滑地衔接起来.因为构造Queue时,可以指定缓冲区的容量,如果队列已满,那么后续的put方法会阻塞.
3. 通过Queue类的task_done的方法来追踪工作进度.有了这个方法,我们就不用像原来那样,进行轮询判断任务是否结束.即使用Queue类的join方法等待队列任务结束.
4. 通过在队列中添加一个特殊的对象,来判断是否到队列尾了.
'''
from queue import Queue
queue = Queue()

def consumer():
    print('Consumer waiting')
    queue.get()
    print('Consumer done')
    
thread = Thread(target=consumer)
thread.start()

print('Producer putting')
queue.put(object())
thread.join()
print('Producer done')

#设置队列缓冲区容量
queue = Queue(1)
def consumer():
    time.sleep(0.1) #给生产者流出一些时间
    queue.get()
    print('Consumer got 1') # run 2
    queue.get()
    print('Consumer got 2') # run 4
    
thread = Thread(target=consumer)
thread.start()

queue.put(object())
print('Product put 1') # run 1
queue.put(object())
print('Product put 2') # run 3
thread.join()
print('Product done')

# 追踪工作进度
in_queue = Queue()
def consumer():
    print('Consumer waiting')
    work = in_queue.get()
    print('Consumer working')
    print('working ...')
    print('Consumer done')
    in_queue.task_done()
    
Thread(target=consumer).start()

in_queue.put(object())
print('Producer waiting')
in_queue.join()
print('Producer done')

# 使用Queue来实现
def download(item):
    print('download:',item)


def resize(item):
    print('resize:',item)


def upload(item):
    print('upload:', item)


class ClosableQueue(Queue):
    SENTINEL = object()

    def close(self):
        self.put(self.SENTINEL)

    def __iter__(self):
        while True:
            item = self.get()
            try:
                if item is self.SENTINEL:
                    return # Cause the thread to exit
                yield item
            finally:
                self.task_done()


class StoppableWorker(Thread):
    '''生产者'''
    def __init__(self,func,in_queue,out_queue):
        super().__init__()
        self.func = func
        self.in_queue = in_queue
        self.out_queue = out_queue
        self.polled_count = 0
        self.work_done = 0


    def run(self):
        for item in self.in_queue:
            result = self.func(item)
            self.out_queue.put(result)


download_queue = ClosableQueue()
resize_queue = ClosableQueue()
upload_queue = ClosableQueue()
done_queue = ClosableQueue()

threads = [
    StoppableWorker(download, download_queue, resize_queue),
    StoppableWorker(resize, resize_queue, upload_queue),
    StoppableWorker(upload, upload_queue, done_queue),
]

for thread in threads:
    thread.start()

for _ in range(10):
    download_queue.put(object())

download_queue.close()
download_queue.join()
resize_queue.close()
resize_queue.join()
upload_queue.close()
upload_queue.join()
print(done_queue.qsize(), 'items finished')
```

### 要点

- 管线是一种优秀的任务处理方式,它可以把处理流程划分为若干阶段,并使用多余python线程来同时执行这些任务
- 构建并发式的管线时,要注意许多问题,其中包括:如何防止某个阶段陷入持续等待的状态之中,如何停止工作线程,以及如何防止内存膨胀等.
- Queue类所提供的机制,可以彻底解决上述问题,它具备阻塞式的队列操作,能够指定缓冲区尺寸,而且还支持join方法,这使得开发者可以构建出健壮的管线.

## 第40条: 考虑用协程来并发地运行多个函数

> 线程有三个显著的缺点:
>
> 1. 为了确保数据安全,我们必须使用特殊的工具来协调这些线程,这使得多线程代码比单线程代码更难懂.使得程序难以维护以及扩展
> 2. 线程需要占用大量内存,每个正在执行的线程,大约占据8MB内存.
> 3. 线程启动时的开销比较大.如果程序不停的依靠创建新线程来同时执行多个函数,并等待这些线程结束,那么使用线程所引发的开销,会拖慢整个程序的速度.
>
> python的coroutine(协程)可以避免上述问题,它使得python程序看上去好像在同时运行多个函数.协程的实现方式,实际上是对生成器的一种扩展.启动生成器协程所需的开销,与调用函数的开销相仿.处于活跃状态的协程,在其耗尽之前,只会占用不到1KB的内存.
>

```python
'''
协程的工作原理:每当生成器函数执行到yield表达式的时候,消耗生成器的那段代码,那就通过send方法给生成器回传一个值.而生成器在收到经由send函数所传进来的这个值之后,将其视为yield表达式的执行结果.
'''
def my_coroutine():
    while True:
        received = yield
        print('Received: ', received)
        
it = my_coroutine()
next(it) #调用一次next函数,以便将生成器推进到第一条yield表达式那里
it.send('First') # send操作,会发送数据给yield表达式作为它的输入
it.send('Second')

'''
实现一个生成器协程:给它依次发送很多数值,该协程每收到一个数值,就会给出当前所统计到的最小值.
'''
def minimize():
    current = yield # 第一次启动的时候,第一个数就是当前最小值;即current保存着当前最小值
    while True:
        value = yield current
        current = min(value, current)
        
it = minimize()
next(it)
print(it.send(10))
print(it.send(4))
print(it.send(22))
print(it.send(-1))
'''
与线程相似,协程也是独立的函数,它可以消耗由外界传进来的输入数据,并产生相应的结果.
与线程不同的是,协程会在生成其函数中的每个yield表达式哪里暂停,直到外界再次调用send函数.
那么接连推进多个独立的生成器,即可模拟出python线程的并发行为,令程序看上去像是同时运行多个函数.

生命游戏
在任意尺寸的二维网格中,每个细胞都处于生存(alive)或空白(empty)状态:
ALIVE='*'
EMPTY='-'
时钟每走一步,生命游戏就前进一步.向前推进时,我们要点算每个细胞周围的那八个单元格,看看该细胞周围有多少个存活的细胞.
本细胞根据相邻细胞的存活数来判断自己在下一轮是继续存活,死亡还是再生.
  0  |  1  |  2  |  3  |  4  
-----|-----|-----|-----|-----
-*---|--*--|--**-|--*--|-----
--**-|--**-|-*---|-*---|-**--
---*-|--**-|--**-|--*--|-----
-----|-----|-----|-----|-----
我们用生成器协程来建模,把每个细胞都表示为一个协程,并令这些协程步调一致地向前推进.
建模:
1. count_neighbors,相邻细胞的生存状态
2. Query对象,作用是向生成器协程提供一种手段,使得协程能够借此向外围环境查询相关的信息.
'''
ALIVE = '*'
EMPTY = '-'
Query = namedtuple('Query',('y','x'))

def count_neighbors(y, x):
    n_ = yield Query(y+1, x+0) # North
    ne = yield Query(y+1, x+1) # Northeast
    e_ = yield Query(y+0, x+1) # East
    se = yield Query(y-1, x+1) # SouthEast
    s_ = yield Query(y-1, x+0) # South
    sw = yield Query(y-1, x-1) # SouthWest
    w_ = yield Query(y+0, x-1) # West
    nw = yield Query(y+1, x-1) # NorthWest
    neighbor_states = [n_, ne, e_, se, s_, sw, w_, nw]
    count = 0
    for state in neighbor_states:
        if state == ALIVE:
            count +=1
    return count

it = count_neighbors(10, 5)
q1 = next(it)
print('First yield: ', q1)
q2 = it.send(ALIVE)
print('Second yield: ', q2)
q3 = it.send(ALIVE)
print('Third yield: ', q3)
q4 = it.send(ALIVE)
print('fourth yield: ', q4)
q5 = it.send(ALIVE)
print('fifth yield: ', q5)
q6 = it.send(ALIVE)
print('sixth yield: ', q6)
q7 = it.send(ALIVE)
print('seventh yield: ',q7)
q8 = it.send(EMPTY)
print('eighth yield: ', q8)
try:
    count = it.send(EMPTY)
except StopIteration as e:
    print('Count: ', e.value)
    
'''
count_neighbors协程把相邻的存活细胞数量统计出来之后,我们必须根据这个数量更新细胞的状态;
定义step_cell的协程,这个生成器会产生Transition对象,用以表示本细胞的状态迁移.

1. step_cell协程通过参数来接收当前细胞的网格坐标.
2. 查询当前细胞的初始状态
3. 运行count_neighnors协程,查看当前细胞附近的其他细胞状态
4.运行game_logic函数,判断本细胞在下一轮应该处于何种状态
'''
Transition = namedtuple('Transition', ('y','x','state'))
def game_logic(state, neighbors):
    pass

def step_cell(y, x):
    state = yield Query(y,x)
    neighnors = yield from count_neighbors(y,x)
    next_state = game_logic(state, neighnors)
    yield Transition(y, x, next_state)
'''
注意:step_cell协程用yield from表达式来调用count_neighbors.
在python程序中,这种表达式可以把生成器协程组合起来,使开发者能够更加方便地复用小段的功能代码,并通过简单的协程来构建复杂的协程.

count_neighbors协程消耗完之后,其最终的返回值(也就是return语句的返回值)会作为yield from表达式的结果,传给step_cell.

接下来,定义游戏的逻辑函数
'''
def game_logic(state, neighnors):
    if state == ALIVE:
        if neighbors<2:
            return EMPTY
        elif neighnors>3:
            return EMPTY
    else:
        if neighbors == 3:
            return ALIVE
    return state

it = step_cell(10, 5)
q0 = next(it)
print('Me:', q0)
q1 = it.send(ALIVE) # Send my state, get neighbor Query
print('First yield: ', q1)
q2 = it.send(ALIVE)
print('Second yield: ', q2)
q3 = it.send(ALIVE)
print('Third yield: ', q3)
q4 = it.send(ALIVE)
print('fourth yield: ', q4)
q5 = it.send(ALIVE)
print('fifth yield: ', q5)
q6 = it.send(ALIVE)
print('sixth yield: ', q6)
q7 = it.send(ALIVE)
print('seventh yield: ',q7)
q8 = it.send(EMPTY)
print('eighth yield: ', q8)
t1 = it.send(EMPTY) # send game decision,get transition
print('Outcome:',t1)
'''
游戏状态转变规则:
1. 本细胞存活时,周围的存活细胞小于2个,本细胞下轮死亡
2. 本细胞存活时,周围的存活细胞大于3个时,本细胞下轮死亡
3. 本细胞死亡时,周围存活细胞等于3个时,本细胞下轮复活

生命游戏的目标是同时在网格中的每个细胞上面运行刚才编写的那套游戏逻辑.
为此,我们把step_cell协程组合到新的simulate协程之中.
新的协程,会多次通过yield from表达式,来推进网格中的每一个细胞.
把每个坐标点中的细胞都处理完之后,simulate协程会产生TICK对象(时钟),用以表示当前这代细胞已经全部迁移完毕.
'''
TICK = object()
def simulate(height, width):
    while True:
        for y in range(height):
            for x in range(width):
                yield from step_cell(y, x)
        yield TICK
        
'''
simulate的好处在于,它和外界环境完全脱离的关联.

接下来,我们需要定义网格对象,同时定义外部代码来处理Query,Transition,TICK值并设置游戏的初始状态.

游戏逻辑
当所有细胞通过step_cell转移到下一个状态的时候,游戏的时钟就会向前走一步.
只要simulate协程还在推进,这个过程就会一致持续下去.
'''
class Grid(object):
    '''代表整张网格的类'''
    def __init__(self, height, width):
        self.height = height
        self.width = width
        self.rows = []
        for _ in range(self.height):
            self.rows.append([EMPTY]*self.width)
            
    def __str__(self):
        result = ''
        for i in range(self.height):
            result+=(''.join(self.rows[i])+'\n')
        return result[:-1]
    
    def query(self, y,x):
        return self.rows[y%self.height][x%self.width] #使用模运算避免坐标越界情况
    
    def assign(self, y, x, state):
        self.rows[y%self.height][x%self.width] = state
        
'''
live_a_generation用于对simulate及其内部的协程所产生的各种值进行解释
'''
def live_a_generation(grid, sim):
    progeny = Grid(grid.height, grid.width)
    item = next(sim)
    while item is not TICK:
        if isinstance(item, Query):
            state = grid.query(item.y, item.x)
            item = sim.send(state)
        else: # Must be a Transition
            progeny.assign(item.y, item.x, item.state)
            item = next(sim)
            
    return progeny

# 初始化一个网格,并设置所有细胞的初始状态
grid = Grid(5,9) 
grid.assign(0,3,ALIVE)
grid.assign(1,4,ALIVE)
grid.assign(2,2,ALIVE)
grid.assign(2,3,ALIVE)
grid.assign(2,4,ALIVE)
print(grid)

'''
现在我们可以逐代推进这张网络,每推进一次,它就变迁到下一代
'''
class ColumnPrinter(object):
    def __init__(self):
        self.all_grids = []

    def append(self, grid_str):
        grid_str = grid_str.split('\n')
        self.all_grids.append(grid_str)

    def __str__(self):
        result = ''
        if not len(self.all_grids):
            return result
        width = int(len(self.all_grids[0][0])/2)
        for i in range(len(self.all_grids)):
            result += ' '*width
            result += str(i)
            result += ' '*width
            result += '|'
        result = result[:-1]+'\n'
        for row_num in range(len(self.all_grids[0])):
            rows = []
            for i in range(len(self.all_grids)):
                rows.append(self.all_grids[i][row_num])
            rows = '|'.join(rows)
            rows = rows[:-1]+'\n'
            result += rows
        return result[:-1]
            
columns = ColumnPrinter()
sim = simulate(grid.height, grid.width)
for i in range(5):
    columns.append(str(grid))
    grid = live_a_generation(grid, sim)
    
print(columns)
'''
以上代码的实现优势:开发者能够在不修改game_logic函数的前提下,更新该函数外围的那些代码.我们可以在现有的Query,Transition,TICK机制的基础上修改相关的规则,或施加影响范围更为广泛的效果.
'''

# Python2的协程;没有yield from表达式,需要多写一层循环
def delegated():
    yield 1
    yield 2
    
def composed():
    yield 'A'
    for value in delegated(): # yield from in Python 3
        yield value
    yield 'B'
    
print(list(composed()))

#第二个限制:python2的生成器不支持return语句,通过try/catch语句来实现相同的行为
class MyReturn(Exception):
    def __init_-(self, value):
        self.value = value
        
def delegated():
    yield 1
    raise MyReturn(2) # return 2 in Python 3,
    yield 'Not reached'
    
def composed():
    try:
        for value in delegated():
            yield value
    except MyReturn as e: #这里捕获异常之后,相当于退出循环,那么delegated函数中的异常后面的语句会不会执行了.
        output = e.value
    yield output*4
print list(composed())
```

### 要点

- 协程提供了一种有效的方式,令程序看上去好像能够同时运行大量函数
- 对于生成器内的yield表达式来说,外部代码通过send方法传给生成器的那个值,就是该表达式所要具备的值
- 协程是一种强大的工具,它可以把程序的核心逻辑,与程序同外部环境交互时所用的代码相分离
- python2不支持yield from表达式,也不支持从生成器内通过return语句向外界返回某个值

## 第41条: 考虑用concurrent.futures来实现真正的平行计算

> 我们可以通过内置的concurrent.futures模块,来利用另外一个叫multiprocessing的内置模块,从而实现这种需求.该做法会以子进程的形式,平行的运行多个解释器,从而令python程序能够利用多核心CPU来提升执行速度.
>
> 由于子进程与主解释器相分离,所以它们的全局解释器锁也是相互独立的.

```python
'''
纳维-斯托克斯方程(Navier-Stokes equation)
'''
def gcd(pair):
    a, b = pair
    low = min(a, b)
    for i in range(low, 0, -1):
        if a%i==0 and b%i==0:
            return i
numbers = [(1963309, 2265973),(2030677, 3814172),
          (1551645, 2229620), (2039045, 2020802)]
start = time()
results = list(map(gcd, numbers))
end = time()
print('Took %.3f seconds' % (end-start))

'''
使用多线程来实现
'''
start = time()
pool = ThreadPoolExecutor(max_workers=2)
results = list(pool.map(gcd, numbers))
end = time()
print('Took %.3f seconds' % (end-start))

'''
可以看到多线程版本的代码运行时间多余单线程;原因是线程启动是有一定的时间开销,与线程池通信也需要时间开销
'''

'''
使用多进程来实现
'''
start = time()
pool = ProcessPoolExecutor(max_workers=2) # the one change
results = list(pool.map(gcd, numbers))
end = time()
print('Took %.3f senconds' % (end-start))
'''
多进程版本的运行速度比以上两个版本都要快;这时因为ProcessPoolExecutor类会利用multiprocessing模块所提供的底层机制,来逐步完成下列操作:
1. 把numbers列表中的每一项输入数据都传给map
2. 用pickle模块对数据进行序列化,将其变成二进制格式
3. 通过本地套接字(local socket),将序列化后的数据从主解释器所在的进程发送给子解释器所在的进程
4. 接下来,在子进程中,用pickle对二进制数据进行反序列化操作,将其还原为python对象
5. 引入包含gcd函数的python模块
6. 各条子进程平行的针对各自的输入数据,来运行gcd函数
7. 对运行结果进行序列化操作,将其转变为字节
8. 将这些字节通过socket复制到主进程中
9. 主进程对这些字节执行反序列化操作,将其还原为python对象
10. 最后,把每条子进程所求的计算结果合并到一份列表之中,并返回给调用者.

以上使用的multiprocessing模块开销比较大.原因在于:主进程与子进程通信的时候,必须对数据进行序列化和反序列化操作.所以对于某些较为孤立,且数据利用率较高的任务来说,这套方案非常合适.
'''
```

> multiprocessing还提供其他一些高级机制,如共享内存(shared memory),跨进程锁定(cross-process lock),队列(queue)和代理(proxy)等.

### 要点

- 把引发CPU性能瓶颈的那部分代码,用C语言扩展模块来改写,即可在尽量发挥python特性的前提下,有效提升程序的执行速度.但是,这样做的工作量比较大,而且可能会引入bug
- multiprocessing模块提供了一些强大的工具.对于某些类型的任务来说,开发者只需编写少量代码,即可实现平行计算
- 若想利用强大的multiprocessing模块,最恰当的做法,就是通过内置的concurrent.futures模块及其ProcessPoolExecutor类来使用它
- multiprocessing模块所提供的那些高级功能,都特别复杂,所以开发者尽量不要直接使用它们.

